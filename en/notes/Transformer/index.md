# Transformer Fundamentals

Deep learning notes on Transformer architecture, attention mechanisms, and related mathematical principles.

<div class="card-grid">

<a href="/notes/Transformer/The_Illustrated_Transformer/note" class="card">
<div class="card-title">ðŸ“– The Illustrated Transformer</div>
<div class="card-meta">Illustrated explanation and detailed guide to Transformer architecture</div>
</a>

<a href="/notes/Transformer/MHA_and_MLA/note" class="card">
<div class="card-title">ðŸ§  MHA and MLA</div>
<div class="card-meta">Comparison of Multi-Head Attention and Multi-Layer Attention mechanisms</div>
</a>

<a href="/notes/Transformer/Softmax/note" class="card">
<div class="card-title">ðŸ”¥ Softmax</div>
<div class="card-meta">Deep understanding of Softmax function principles and applications</div>
</a>

</div>
