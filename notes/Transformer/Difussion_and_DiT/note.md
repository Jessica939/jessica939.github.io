# Difussion，U-Net and DiT
在 Stable Diffusion 1.5/2.0 和早期 Midjourney 统治 AI 绘画的时代，**扩散模型 + U-Net**就是生成式视觉领域的绝对黄金标准。
## U-Net
U-Net 最早诞生于 2015 年，初衷是为了做**医学图像分割**（比如把 X 光片里的肿瘤区域精确地描绘出来）。它的名字来源于其网络拓扑结构画出来像一个大写的英文字母 **“U”**。

#### U-Net 的核心结构拆解：

U-Net 由三大部分组成：**编码器（左半边）、解码器（右半边）、跳跃连接（中间的桥梁）**。

**1. 编码器（Encoder / 下采样过程）——“看懂这是什么”**
*   **操作**：使用卷积神经网络（CNN）对输入的噪声图进行不断的“压缩”（池化/下采样）。
*   **效果**：图片的长宽越来越小（比如从 256 缩小到 128、64、32、16），但通道数（厚度）越来越大。
*   **意义**：在这个过程中，网络丢弃了无用的空间细节，提取出了高级的**全局语义特征**。它看懂了“这团噪点的大致轮廓是一只猫，甚至左边是头，右边是尾巴”。（但这时的特征图太小了，没法直接输出）。

**2. 解码器（Decoder / 上采样过程）——“恢复到多大”**
*   **操作**：把编码器压缩出来的高级特征，通过反卷积（或插值）一步步“放大”回原始分辨率。
*   **效果**：特征图的长宽越来越大，最后恢复到 256x256。
*   **痛点**：由于在压缩（编码）的时候，很多像素级别的细节（比如猫的胡须在哪里，毛发的纹理）已经被丢弃了。如果只靠解码器强行放大，生成的图会非常模糊。

**3. 跳跃连接（Skip Connections）**
*   这是 U-Net 的灵魂。为了解决解码器放大后图像模糊、缺乏细节的问题，U-Net 在“U”型的左边（编码器）和右边（解码器）之间架起了桥梁。
*   **做法**：在解码器放大的每一步，直接把**编码器同一层级的原始特征图拿过来，跟当前的特征图拼在一起（Concatenate）**。
*   **意义**：这相当于在考试时（解码），虽然你在做高级逻辑推理，但旁边直接放着一本带有最原始细节的字典（编码器传来的高分辨率特征）。高级语义指导着“画什么”，而跳跃连接传来的低级细节指导着“在哪里画”。
*   有了它，U-Net 能够实现**像素级的精准预测**。

## 扩散模型

### 训练

在这个阶段，我们手里有成千上万张高清原图（如一张高清的“柯基犬”照片），我们要训练 U-Net 成为一个顶级的“去噪专家”。

1. 准备数据
   *   **原图 $x_0$**：一张清晰的柯基照片。
   *   **随机时刻 $t$**：系统随机挑一个时间点，比如 $t=600$（假设总共 1000 步）。
   *   **噪声 $\epsilon$**：系统生成一张和原图一样大的**纯高斯噪声**（比如正态分布产生的随机杂点）。
   *   **合成脏图 $x_t$**：根据数学公式，把原图和噪声混合。
       *   在 $t=600$ 时，图片大概是“30% 的柯基 + 70% 的噪声”。肉眼看过去，只能隐约看到狗的轮廓，大部分是雪花点。

2. U-Net 登场
    我们要给 U-Net 出一道题：
    *   **输入（Input）**：
        1.  那张 $t=600$ 的脏图 $x_t$。
        2.  告诉它现在的时刻 $t=600$（通过 Time Embedding）。
        3.  告诉它这张图的内容文本 $c$：“一只柯基犬”（通过 Text Encoder 转换成向量）。
    *   **任务**：**“请告诉我，刚才我往这张图里加了什么样的噪声？”**

3. 预测与惩罚（Loss Calculation）
   * **U-Net 的输出**：U-Net 会输出一张它认为的**“预测噪声”** $\epsilon_{\theta}$。
   *   **判卷子**：系统拿 U-Net 猜出来的“预测噪声”，跟我们一开始通过数学公式生成的“真实噪声” $\epsilon$ 进行对比（做减法）。
   *   **梯度下降（Backpropagation）**：
       *   如果猜得准，给奖励。
       *   如果猜得不准（比如把狗耳朵当成了噪点，或者没把真正的噪点找出来），就罚它，修改 U-Net 内部的参数权重。

**重复以上过程几十亿次**，让 U-Net 阅图无数。最终，U-Net 练就了火眼金睛：只要你给它一张脏图、一个时间步、一段文字，它就能精准地指出**哪些像素是原本的内容，哪些像素是后来加的噪点**。


### 推理

现在模型训练好了。用户输入了 Prompt：“一只戴墨镜的赛博朋克猫”。
- 步骤 0：初始状态
   *   系统生成一张**完全随机的纯噪声图**（Latent Noise），假设它是 $x_{1000}$。
   *   这就好比一块还没有雕刻的**大理石**，或者一团毫无意义的云。

- 步骤 1：第一刀（$t=1000 \to 999$）
  *   **输入给 U-Net**：
      1.  纯噪声图 $x_{1000}$。
      2.  时间步 $t=1000$。
      3.  文本条件：“一只戴墨镜的赛博朋克猫”。
  *   **U-Net 的思考**：
      *   U-Net 此时很懵，因为它看到的全是杂点。
      *   但是，**Cross-Attention（交叉注意力）** 机制开始起作用了。文本提示词告诉它：“嘿，你要找猫的特征！”
      *   U-Net 勉强在随机杂点里“脑补”出了一些不像猫的杂点。它说：“既然你要猫，那我觉得这些这些像素看起来不像猫，应该是噪声。”
  *   **去噪操作**：
      *   算法拿 U-Net 预测出的噪声，从 $x_{1000}$ 里减去了一小部分。
      *   **结果**：得到了 $x_{999}$。虽然这时候看起来还是一团乱麻，但在数学层面上，它已经比纯噪声稍微“有序”了一丁点。

- 步骤 2：循环
这个过程重复几十次甚至上百次（取决于你的设置，比如 20 步或 50 步）。

    *   **到了 $t=500$ 时**：
        *   此时的 $x_{500}$ 已经能隐约看出轮廓了。
        *   **U-Net 的 U 型结构开始发威**：
            *   **Down Block（下采样）** 看到：“中间有个圆乎乎的东西，可能是头。”
            *   **Cross-Attention** 再次确认：“文本说了是赛博朋克，所以这个头旁边应该有霓虹灯的反光。”
            *   **Skip Connection（跳跃连接）** 把 $x_{500}$ 的具体纹理位置传给 Decoder。
            *   **Up Block（上采样）** 结合以上信息，预测出覆盖在“猫头”和“霓虹灯”上的那层薄薄的噪点。
        *   算法减去这层噪声，画面变得更清晰。

- 步骤 3：最后修饰（$t \to 0$）
  *   当 $t$ 接近 0 时，图像已经非常清晰了。
  *   U-Net 此时的任务不再是“找轮廓”，而是“修细节”。比如猫毛的质感、墨镜的反光、背景的微小颗粒。
  *   它精确地指出最后一点点微小的噪点，算法将其剔除。

- 步骤 4：出图（VAE Decode）
  *   整个过程通常是在**Latent Space（潜空间）**里进行的（为了省算力，处理的是被压缩的数据）。
  *   最后，我们需要用 **VAE Decoder** 把这个去噪完成的“压缩数据”放大、解压回像素空间。
  *   **Boom！** 一张从未存在于世界上的“戴墨镜赛博朋克猫”出现在你屏幕上。



## DiT
**DiT（Diffusion Transformers）** 是生成式 AI 领域近年来最具革命性的架构之一。它由加州大学伯克利分校的 William Peebles 和纽约大学的 Saining Xie 在 2022 年底提出（被 ICCV 2023 收录）。

简单来说，**DiT = 扩散模型（Diffusion） + Transformer**。它用 Transformer 替换了传统扩散模型（如 Stable Diffusion 1.5/2.0、Midjourney 早期版本）中一直使用的 **U-Net** 主干网络。

OpenAI 震惊世界的视频生成模型 **Sora**，以及目前开源图像生成的标杆 **Stable Diffusion 3 (SD3)**，底层的核心架构都是 DiT。

为了让你深入理解，我们将从它的**背景动机、核心架构、关键创新、为什么强大以及实际应用**五个维度进行硬核拆解。


### 一、 为什么需要 DiT？


**U-Net 的局限性：**
1.  **局部感受野**：CNN 的卷积核只能看到局部特征，虽然 U-Net 通过下采样获取了全局信息，但对于长距离依赖（比如图像左上角的物体和右下角的物体如何互动）的建模能力依然不如 Transformer 的全局自注意力机制。
2.  **扩展性（Scalability）遭遇瓶颈**：随着模型参数量的增加，U-Net 的性能提升边际效应递减。相比之下，大语言模型（LLM）已经证明了 Transformer 具有近乎完美的 **Scaling Law（缩放定律）**——只要算力和数据跟上，模型变大，性能就会线性且稳定地提升。

既然 Transformer 在 NLP（GPT-4）和视觉识别（ViT）中都证明了自己，为什么不用它来做扩散模型的生成呢？DiT 应运而生。

### 二、 DiT 的核心架构
DiT 的整体流程借鉴了 **ViT (Vision Transformer)** 和 **LDM (Latent Diffusion Models)**。它的运行是在“潜在空间（Latent Space）”中进行的。

#### 1. 空间压缩（与 Stable Diffusion 类似）
为了减少计算量，DiT 首先使用一个预训练的 VAE（变分自编码器）将像素级的图像（比如 $256 \times 256 \times 3$）压缩到一个低维的潜在空间（Latent Space，比如 $32 \times 32 \times 4$）。

#### 2. 图像切块（Patchification）
传统 U-Net 是把这个 $32 \times 32$ 的特征图当成图片来做卷积。而 DiT 则像 ViT 一样，把它**切成一个个小方块（Patches）**。
*   假设 Patch 大小是 $2 \times 2$，那么 $32 \times 32$ 的特征图就会被切成 $256$ 个 Patches。
*   这 256 个 Patches 经过线性映射（Linear Projection）后，变成一个长度为 256 的 **Token 序列**。
*   这下，图像彻底变成了类似自然语言的“句子”。

#### 3. 核心挑战：如何注入条件（Conditioning）？
在扩散模型中，去噪网络必须知道两个关键信息：
1.  **时间步 $t$ (Timestep)**：当前去噪到了哪一步？（噪声有多大）。
2.  **类别或文本 $c$ (Condition)**：我们要生成什么？（比如猫、狗的类别，或者一段文本 prompt）。

如何把 $t$ 和 $c$ 自然地融入 Transformer 中？这是 DiT 论文最核心的贡献。作者尝试了四种方式：
*   **In-context Conditioning**：把 $t$ 和 $c$ 当作额外的 Token，直接拼在图像序列的最前面（类似 CLS token）。
*   **Cross-Attention**：在 Transformer block 中增加一层交叉注意力机制，让图像 token 去查询 $t$ 和 $c$ 的信息（Stable Diffusion 1.5 用的就是这种思路）。
*   **adaLN (Adaptive Layer Normalization)**：不改变 Token，而是在特征归一化时动手脚。
*   **adaLN-Zero**：在 adaLN 的基础上进一步改进。

#### 4. 最佳方案：adaLN-Zero
最终，论文发现 **adaLN-Zero** 效果最好，计算代价也小。它的运作方式是：
*   把时间步 $t$ 和类别 $c$ 的向量相加（或拼接），输入到一个简单的多层感知机（MLP）中。
*   这个 MLP 会预测出 6 个参数，用于控制 Transformer block 里的 **LayerNorm（层归一化）** 的缩放因子（Scale）和偏移量（Shift）。
*   **为什么叫 Zero？** 因为它在初始化时，强制把残差连接（Residual Connection）之前的缩放参数设为 **0**。这意味着在训练的最开始，整个 DiT Block 就像是一个恒等映射（Identity Function，输入什么就输出什么）。
*   **意义**：这种“零初始化”极大地稳定了深层 Transformer 的训练，是 DiT 能够成功收敛的关键。

#### 5. 还原输出（Unpatchification）
经过 N 个 DiT 块的自注意力计算后，Token 序列被重新“折叠”回原本的空间形状（$32 \times 32 \times 4$），输出预测的噪声，最后再由 VAE 解码成真实的像素图像。


### 三、 DiT 为什么如此强大？

#### 1. 完美的 Scaling Law（缩放定律）
这是 DiT 被 OpenAI 选作 Sora 底层架构的最根本原因。论文通过严谨的实验证明：
*   **增加模型深度和宽度**（参数量从 DiT-S 到 DiT-XL）。
*   **减小 Patch Size**（从 $8 \times 8$ 减小到 $2 \times 2$，意味着 Token 序列变长，计算量指数增加）。
以上两种方法只要增加计算量（GFLOPs），生成的图像质量（FID 分数）就会呈现**极其可预测的、稳定的线性提升**。U-Net 很难做到这一点，而 DiT 证明了“大力出奇迹”在视觉生成领域同样奏效。

#### 2. 全局视野下的物理规律理解
Transformer 的核心是 Self-Attention（自注意力），这意味着序列中的任何一个 Patch，在第一层就能和画面中最远的另一个 Patch 产生直接联系。
这种能力在**视频生成（如 Sora）**中变得极其重要。因为视频包含了时间和空间的双重维度，U-Net 的卷积核很难在时间和空间的长距离上保持事物的一致性（比如一个人走到被遮挡物后，几秒后再走出来）。DiT 的全局感知使其具有了涌现出“模拟物理规律”的潜力。

#### 3. 极高的模态灵活性
U-Net 强绑定了 2D 空间的网格（Grid）结构。如果你想处理不同分辨率、不同宽高比、或者是视频（3D），你需要对 U-Net 结构做大改（比如加 3D 卷积、时序注意力）。
对于 DiT 来说，**一切皆 Token**。
不管是高分辨率图片、长短不一的视频，甚至加入音频，只要你能把它切分并 Patch 化为 Token，DiT 统统可以作为统一的架构照单全收（Sora 正是通过 Spacetime latent patches 处理任意比例和长度的视频）。


#