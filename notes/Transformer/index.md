---
lastUpdated: false
---

# Transformer基础

Transformer架构、注意力机制及相关数学原理的深度学习笔记。

<div class="card-grid">

<a href="/notes/Transformer/The_Illustrated_Transformer/note" class="card">
<div class="card-title">📖 The Illustrated Transformer</div>
<div class="card-desc">Transformer架构图解与详细讲解</div>
<div class="card-meta">2026-02-13</div>
</a>

<a href="/notes/Transformer/MHA_and_MLA/note" class="card">
<div class="card-title">🧠 MHA and MLA</div>
<div class="card-desc">多头注意力与多层注意力机制对比</div>
<div class="card-meta">2026-02-26</div>
</a>

<a href="/notes/Transformer/Softmax/note" class="card">
<div class="card-title">🔥 Softmax</div>
<div class="card-desc">深度理解Softmax函数原理与应用</div>
<div class="card-meta">2026-02-26</div>
</a>

<a href="/notes/Transformer/MoE/MoE" class="card">
<div class="card-title">🌀 MoE</div>
<div class="card-desc">混合专家模型：从1991到2024的演进与工程挑战</div>
<div class="card-meta">2026-02-27</div>
</a>

</div>
