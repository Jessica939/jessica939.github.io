# From MHA to MLA
联系[多头注意力（MHA）](..\The_Illustrated_Transformer\note.md#多头注意力mha)
## MHA 与 KV Cache 的物理灾难

在大模型推理生成时，是一个词一个词生成的（自回归）。
假设我们已经生成了 1000 个词，现在要生成第 1001 个词。
*   **数学逻辑**：第 1001 个词的 $Q_{1001}$，必须去和前面 1000 个词的 $K_{1...1000}$ 做点积，然后再去乘以 $V_{1...1000}$。
*   **工程痛点（KV Cache）**：为了不把前面 1000 个词重新算一遍，系统会把前面 1000 个词的 $K$ 和 $V$ 矩阵永远保留在显存里。这叫 **KV Cache**。


假设模型有 $h=128$ 个注意力头（Heads），每个头的维度是 $d=128$。
*   每一个词，它的 $K$ 和 $V$ 的维度都是 $128 \times 128 = 16384$。
*   用 FP16 存，**每一个词就要占用 64 KB 的显存**。
*   如果是 10 万字的长上下文（100k Context），仅仅**一句话的 KV Cache 就要占去 6.4 GB 显存！** 如果并发 100 个用户（Batch Size = 100），单单是存这些历史记录，就需要 **640 GB 显存（需要 8 张 A100 80G 才能存下！）**。

**结论**：在 MHA 下，大模型的算力根本没用完，全在等显存读写 KV Cache，这叫 **Memory-bound**。


## MLA的数学破局

这是 DeepSeek V2/V3 封神的绝招。
既然 $K$ 和 $V$ 太大了存不下，DeepSeek团队想出了一个绝妙的数学 Trick：**低秩联合压缩（Low-Rank Joint Compression）。**

既然多头注意力里,信息其实是有冗余的，我们为什么要把 $h$ 个头的 $K$ 和 $V$ 全存下来呢？

### 1. 压缩阶段（Down-projection）
对于输入的第 $t$ 个词的隐含特征 $h_t$（维度很高，比如 7168）：
MLA 不去生成庞大的多头 $K_t$ 和 $V_t$。相反，它用一个极其瘦长的矩阵 $W_{down}$，把 $h_t$ **“降维压缩”** 成一个非常短的隐向量（Latent Vector），叫做 **$c_t^{KV}$**。

*   **$c_t^{KV}$ 的维度可能只有 512。**
*   **核心颠覆**：在显存的 KV Cache 里，**我们再也不存庞大的 $K$ 和 $V$ 了，我们只存这个短短的 512 维的 $c_t^{KV}$！**
*   **物理收益**：KV Cache 瞬间被压缩到了原来的几十分之一！

### 2. 恢复阶段（Up-projection）
你要算 Attention，没有 $K$ 和 $V$ 怎么算？
等需要计算的时候，再用另外两个矩阵 $W_k^{up}$ 和 $W_v^{up}$，把这个短小的 $c_t^{KV}$ **“广播/恢复”** 回高维的多头 $K$ 和 $V$。

*   公式： $K_t = W_k^{up} \cdot c_t^{KV}$
*   公式： $V_t = W_v^{up} \cdot c_t^{KV}$


## 矩阵吸收推导


> 等等！如果我在计算 Attention 的时候，还要用矩阵乘法把 $c_t^{KV}$ 恢复成高维的 $K_t$ 和 $V_t$，那我虽然省了长期存储的空间，但在计算的那一瞬间，SRAM/显存里不还是会瞬间爆出巨大的 $K$ 和 $V$ 吗？计算量不反而变大了吗？

这正是 MLA 全文最精华的数学魔法——利用矩阵乘法的结合律，把恢复矩阵吸收掉！

传统的 Attention 分数是 $Q_t$ 和 $K_t$ 的点积。
在 MLA 中：
*   $Q_t = W_q \cdot c_t^{Q}$
*   $K_t = W_k^{up} \cdot c_t^{KV}$

我们要算它们的点积：
$$ \text{Score} = Q_t \cdot K_t $$
$$ \text{Score} = (W_q \cdot c_t^{Q}) \cdot (W_k^{up} \cdot c_t^{KV}) $$
我们把括号拆开，重新结合！
$$ \text{Score} = c_t^{Q} \cdot \left( W_q^T \cdot W_k^{up} \right) \cdot c_t^{KV} $$

你看中间那一坨是什么？**$W_q^T \cdot W_k^{up}$，这俩全都是模型训练好的、固定的权重矩阵！**
既然是常数矩阵，我们完全可以在模型加载的时候，**提前把它们乘起来（Offline Absorption）**，变成一个融合权重矩阵 $W_{QK}$。

于是，在线推理计算分数的公式变成了：
$$ \text{Score} = (c_t^Q \cdot W_{QK}) \cdot c_t^{KV} $$


在这个最终公式里，**庞大的多头 $K_t$ 彻底消失了！永远不需要被恢复出来！**
系统只需要拿着短短的 $c_t^{KV}$，直接去和处理过的 Query 点积，就能得到完全相同的 Attention 分数！

同理，计算 $V$ 的时候，也可以把 $W_v^{up}$ 吸收到输出投影矩阵（Out-Projection）里，**庞大的 $V_t$ 也在物理世界上彻底消失了！**



## 位置编码RoPE该怎么办？
联系[使用位置编码来代表序列顺序](..\The_Illustrated_Transformer\note.md#使用位置编码来代表序列顺序)
*   **RoPE 的本质**：它是一种特殊的旋转矩阵。它必须在 $Q$ 和 $K$ 生成之后，逐个元素地乘上去，用来表示词与词之间的距离。
*   **MLA 的噩梦**：如果我们把 $K$ 压缩成了隐向量 $c_t^{KV}$，并且用结合律把 $K$ 给吸收了，那 RoPE 这个旋转矩阵往哪里乘？如果往 $c_t^{KV}$ 上乘，数学上是不等价的；如果不乘，模型就丧失了位置信息，变成瞎子。

*   **DeepSeek 的解法（RoPE Decoupling）**：
    他们强行在维度上切了一刀：
    $K$ 一分为二 =[被压缩的隐向量 $c_t^{KV}$] +[一小段不被压缩、专门用来挂载 RoPE 旋转矩阵的 $K_{rope}$]。
    这就保证了既能享受极高比例的压缩，又能在数学上保留绝对精准的位置旋转信息。
    （这也是为什么如果在 3D 视频 DiT 里做 3D RoPE 解耦，难度极大，因为高维旋转矩阵的解耦极其复杂）。

